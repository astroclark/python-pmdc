#!/usr/bin/env python
# -*- Python -*-
#
# poor man's diskcache
#
# a self-contained python program that scans directories for frames
from bisect import bisect_left
from itertools import chain
from os.path import abspath, dirname, exists, join as os_path_join, getmtime
from sys import argv, exit, stdout, stderr

import atexit
import cPickle as pickle
import datetime
import gc
import glob
import os
import shelve
import signal
import stat
import tempfile
import time

VERSION=(1,0,1)
__version__='.'.join(map(str, VERSION))
__author__="Jeffery Kline"
__copyright__="Copyright 2013, Jeffery Kline"
__license__="GPL"
__email__="jeffery.kline@gmail.com"

# human-readable constants for the dc key. For Python versions>2.6 use a 
# named tuple instead.
# see also http://code.activestate.com/recipes/500261/
DIRNAME=0
SITE=1
FT=2
DUR=3
EXT=4

pickle_dumps = pickle.dumps
pickle_loads = pickle.loads
pickle_load = pickle.load
pickle_dump = pickle.dump

def segment_add(seg, sl):
    """
    add segment seg = [start, end] to segment list in place. Segment
    list sl described by list of pairs [[s0, e0], ...]

    NB. This assumes sl is sorted and start < end (these conditions
    are not checked)

    This is (essentially) copied from Kipp Cannon's original GPL'ed
    glue.segments

    It is also is a very expensive operation.
    """
    idx = bisect_left(sl, seg)
    sl.insert(idx, seg)
    n = len(sl)

    # assuming sl was created using segment_add(!), we need to
    # consider elements starting at the left-neighbor of seg to the
    # end. The left-neighbor could have a very large end time, and we
    # should coalesce it.
    i = j = max(0, idx-1)
    while j < n:
        lo, hi = sl[j]
        j += 1
        while j < n and hi >= sl[j][0]:
            hi = max(hi, sl[j][1])
            j += 1

        if lo < hi:
            sl[i] = (lo, hi)
            i += 1
    del sl[i:]

def parse_lfn(lfn):
    "return tuple (site, frametype, (int) gpsstart, (int) dur)"
    site,frametype,gpsstart_s,dur_ext = lfn.split('-')
    gpsstart = int(gpsstart_s)
    dur_s, ext = dur_ext.split('.')
    dur = int(dur_s)
    return (site, frametype, gpsstart, dur, ext)

def parallel_update_dc(dc, args, hot, concurrency, tempdir):
    """Update dc in parallel by forking off 1 'pmdc.py' process per
    entry in args, running at most 'concurrency' processes at one
    time.

    ipc with the forked processes is done by writing to temporary
    files on disk.
    """
    from shutil import rmtree
    from subprocess import PIPE, Popen
    from tempfile import mkdtemp
    from time import sleep
    
    # the process list
    proc_l = []
    # each process writes to a file.
    fname_l = []

    # Block until fewer than 'nalive' processes are alive.  Raise
    # error if any proces exited abnormally.
    def proc_l_pause(concurrency=0):
        alive_l = lambda: [p for p in proc_l if p.poll() is None]
        failed_l = lambda: [p for p in proc_l if p.poll()]
        max_alive = max(concurrency-1, 0)

        while len(alive_l()) > max_alive:
            # catch any subprocesses that exited with an error
            for failed in failed_l():
                so, se = failed.communicate()
                raise RuntimeError(se)
            sleep(0.125)

    # remove files created, destroy lingering processes
    tempd = mkdtemp(dir=tempdir)
    def proc_l_kill():
        for p in proc_l:
            try: os.kill(p.pid, signal.SIGKILL)
            except OSError: pass

    atexit.register(rmtree, tempd)
    atexit.register(proc_l_kill)

    # create the subprocesses
    for _d in args:
        fname_l.append(os_path_join(tempd, str(len(proc_l))))
        # call pmdc with same cache and 1 directory argument. Write
        # new information to ipc file.
        cmd = [argv[0], argv[1], _d, "-i", fname_l[-1]]
        proc_l.append(Popen(cmd, stdout=stdout, stderr=stderr))
        # block until some subprocess exits
        proc_l_pause(concurrency)
    # block until all subprocesses exit
    proc_l_pause()

    # update dc and hot using ipc_file
    for fname in fname_l:
        _cache = pickle_load(open(fname, 'r'))
        hot.update(_cache["hot"])
        # dc is a shelve object; this loop is required
        for key, val in _cache["dc"].iteritems():
            dc[key] = val

def update_dc(dc, d, hot):
    """
    modify the dictionary dc to reflect the "diskcache content of
    d". Skip 'hot' directories that have not been updated.  
    
    Calls sem.release() before returning

    NB: calls os.walk(d)
    """
    # ignore hot directories that have not changed
    try: 
        if getmtime(d) <= hot[d]: 
            return
    except KeyError: 
        pass

    # store all new information in keys_seen. Modify dc at the very
    # end.  dc might be shelve object; keys must be strings, and
    # updating should be explicit.
    keys_seen = {}
    for dirpath, dirname_l, filename_l in os.walk(d):
        # remove hot directories from the list to traverse
        for _dir in list(dirname_l):
            _fulldir = os_path_join(dirpath, _dir)
            try:
                # remove from list to  skip _dir on next iteration
                if getmtime(_fulldir) <= hot[_fulldir]:
                    dirname_l.remove(_dir)
            except KeyError:
                pass

        # catch empty 'hot' directories
        if not dirname_l and not filename_l:
            hot[dirpath] = getmtime(dirpath)
        for f in filename_l:
            try:
                (site, frametype, gpsstart, dur, ext) = parse_lfn(f)
            except:
                continue

            if dirpath not in keys_seen:
                keys_seen[dirpath] = {}
                hot[dirpath] = getmtime(dirpath)

            sfde = (site, frametype, dur, ext)
            seg = (gpsstart, gpsstart + dur)
            if sfde not in keys_seen[dirpath]:
                keys_seen[dirpath][sfde] = []
            segment_add(seg, keys_seen[dirpath][sfde])
    for dirpath in keys_seen:
        dc[dirpath] = keys_seen[dirpath]

def write_dc(dc, hot, protocol, extension, fh):
    "write the diskcache to filehandle fh in various formats"
    if protocol == "ldas":
        # mimic the ldas format
        #
        # /foo/bar,site,frametype,1,dur timestamp nfiles {120000 122272 122304 130016}
        text = []
        import re
        notalpha_sub = re.compile('[\W]+').sub
        for dirname in dc:
            for site, frametype, dur, ext in dc[dirname]:
                if ext not in extension:
                    continue
                segmentlist = dc[dirname][(site, frametype, dur, ext)]
                sl_s = '{' + notalpha_sub(' ', str(segmentlist)).strip() + '}'
                key_s = ','.join((dirname, site, frametype, '1', str(dur)))
                mtime_s = str(int(hot[dirname]))
                nfile_s = str(sum(s1-s0 for s0, s1 in segmentlist)/dur)
                text.append(' '.join((key_s, mtime_s, nfile_s, sl_s)))
        text.sort()
        fh.write('\n'.join(text))
        fh.write('\n')
    elif protocol == "pmdc":
        text = []
        import re
        notalpha_sub = re.compile('[\W]+').sub
        for dirname in dc:
            for site, frametype, dur, ext in dc[dirname]:
                if ext not in extension:
                    continue
                segmentlist = dc[dirname][(site, frametype, dur, ext)]
                sl_s = '{' + notalpha_sub(' ', str(segmentlist)).strip() + '}'
                key_s = ','.join((dirname, site, frametype, 'x', str(dur), ext))
                mtime_s = str(int(hot[dirname]))
                nfile_s = str(sum(s1-s0 for s0, s1 in segmentlist)/dur)
                text.append(' '.join((key_s, mtime_s, nfile_s, sl_s)))
        text.sort()
        fh.write('\n'.join(text))
        fh.write('\n')
    elif protocol == "dcfs":
        # the file list of dcfs looks like this:
        # 
        # extension/
        # extension/frame-type/
        # extension/frame-type/site
        # extension/frame-type/site/gps[:-5]
        # extension/frame-type/site/gps[:-5]/site-ft-gps-dur.ext
        # 
        # to get this list quickly, we use dicts as follows:
        # 
        # extension -> frame-types
        # (extension, frame-types) -> site
        # (extension, frame-types, site) -> (dirname, duration, segmentlist)
        from collections import defaultdict
        d_ext = defaultdict(set)
        d_ext_ft = defaultdict(set)
        d_ext_ft_site = defaultdict(list)
        for dirname in dc:
            for site, frametype, dur, ext in dc[dirname]:
                if ext not in extension:
                    continue
                d_ext[ext].add(frametype)
                d_ext_ft[(ext, frametype)].add(site)
                d_ext_ft_site[(ext, frametype, site)].append(
                    (dirname, dur, dc[dirname][(site, frametype, dur, ext)]))
        pickle_dump(d_ext, fh)
        pickle_dump(d_ext_ft, fh)
        pickle_dump(d_ext_ft_site, fh)
    else:
        raise ValueError("Unknown protocol %s" % str(protocol))

if __name__=="__main__":
    from optparse import OptionParser
    usage = "usage: %prog <cache namespace> [<directory list>] [options]" 

    description = ' '.join("""Scan each directory in <directory list>
    for frame files.  Write text files atomicly (this is useful for
    large writes and other processes that need to read valid files).
    Use a lock file to prevent more than one scan that depends on
    <cache namespace> from occuring at the same time (this is useful
    for cron jobs).""".split())

    version = "%%prog %s" % __version__

    parser = OptionParser(usage=usage, description=description, version=version)
    extension_help = ' '.join("""[[]] Scan for files ending with
                               ".EXTENSION". Passing "--extension gwf"
                               is equivalent to not passing an
                               extension flag.  Use multiple times for
                               multiple extensions. The dot is not
                               part of EXTENSION, do not include it.""".split())
    parser.add_option("-e", "--extension", action="append", default=[],
                      help=extension_help)

    output_help = ' '.join(
        """[-] Name of file to write diskcache output to.  Use '-' for
           stdout. If a filename provided, then the write operation is
           guaranteed to be atomic (manual manipulation of temporary
           files is not required).  Only applies if used in conjuction
           with -p flag.""".split())
    parser.add_option("-o", "--output", default="-", help=output_help)

    status_help = "[0644] Numeric mode of atomicly-written files."
    parser.add_option("-m", "--output-file-mode", type="int", default=0644,
                      help=status_help)

    ipc_help = ' '.join("""[None] Name of file for internal process
    communication. If used, then directory list must have length 1.
    """.split())
    parser.add_option("-i", "--ipc-file", default=None, help=ipc_help)

    protocol_choices = ("dcfs", "ldas", "pmdc")
    protocol_help = ' '.join(
        """[None] Protocol of output to write. Choose from %s.  Use
        'ldas' for compatibility with ldas-tools.  Use 'pmdc' for an
        extended protocol that includes file extension information.
        Formats 'ldas' and 'pmdc' are plain text and results are
        sorted. 'dcfs' is a format useful for the diskcache
        filesystem.""".split() ) % str(protocol_choices)
    parser.add_option("-p", "--protocol", choices=protocol_choices,
                      help=protocol_help)
    concurrency_help = ' '.join(
        """[5] Maximum number of concurrent scan processes. Only
           applicable if more than 1 directory is passed as a
           positional argument.""".split())
    parser.add_option("-r", "--concurrency", type="int", default=5,
                      help=concurrency_help)


    tempdir_help = ' '.join(
        """[None] Temporary directory. Only applicable if more than 1
           directory is passed as a positional argument. It is used by
           forked processes which use it for IPC.""".split())
    parser.add_option("-t", "--tempdir", default=None, help=tempdir_help)

    status_help = "Print some info and exit."
    parser.add_option("-s", "--status", action="store_true", default=False,
                      help=status_help)

    (options, args) = parser.parse_args()

    # args requires cache_file and a list of 0 or more directories.
    # Branch depending on the case at hand.  If directory list has 1
    # element then run in main thread.  If directory list is longer,
    # fork one process for each entry.
    if not args:
        parser.error("No cache provided")
        exit(1)
    
    # this is the cache namespace
    cache_file = args[0]

    # set up lock file for master process to protect 1 writer
    if not options.ipc_file: 
        lock_file = cache_file + '.lock'
        if exists(lock_file):
            raise RuntimeError("Lock file %s exists." % abspath(lock_file))
        open(lock_file, 'w')
        atexit.register(os.unlink, lock_file)

    # cache_file holds the 'hot' dict and some other info that is used
    # later.  After this point, the cache_file is assumed to exist and
    # contain two keys: "hot" and "header".
    #
    # (dict) hot:
    #    key: dirpath
    #    value: mtime(dirpath)
    # 'hot' dirs are directories that have been completely
    # indexed. This includes empty dirs.
    #
    # The value of mtime was sampled when (approximately) the last new
    # dc key containing dirpath was added.
    try: 
        cache_file_fh = open(cache_file, 'r')
        initial_run = False        # has the cache_file been created?
    except: 
        initial_run = True
        header = {"initial run": initial_run, "timestamp": datetime.datetime.today()}
        pickle_dump({"hot": {}, "header": header}, open(cache_file, 'w'))
        cache_file_fh = open(cache_file, 'r')

    if options.status:
        header= pickle_load(cache_file_fh)["header"]
        for kv in header.iteritems():
            print "%s: %s" % kv
        age = datetime.datetime.today() - header['timestamp']
        print "state age: %s" % str(age)
        exit(0)

    hot = pickle_load(cache_file_fh)["hot"]
    cache_file_fh.close()
    shelve_file = cache_file + ".shlv"
    
    # If using interprocess communication, then dc must be empty so
    # that only new state is saved to disk. Otherwise, populate dc from cache.
    # 
    # (shelve,dict) dc:
    #   key: pickle.dumps((dirpath, site, frametype, dur, ext)) 
    #   value: segmentlist associated with key
    if options.ipc_file is None:
        dc = shelve.open(shelve_file)
    else:
        # check that ipc_file was called with length-1 directory list
        if len(args)>2:
            raise RuntimeError("Cannot use ipc-file with more than 1 directory")
        dc = {}

    start_scan_t = time.time()
    if len(args) == 2:
        update_dc(dc, args[1], hot)
    elif len(args) >2:
        parallel_update_dc(dc, args[1:], hot, options.concurrency, options.tempdir)
    end_scan_t = time.time()

    # write atomicly. alternative output formats if requested to
    # desired output
    start_write_t = time.time()
    if options.protocol is not None:
        if options.output == "-":
            fh = stdout
        else:
            fh = tempfile.NamedTemporaryFile(dir=dirname(options.output))
        extension = set(options.extension)
        if not extension:
            extension.add('gwf')
        write_dc(dc, hot, options.protocol, extension, fh)
        
        # finish the atomic write
        if fh != stdout:
            fh.flush()
            os.chmod(fh.name, options.output_file_mode)
            os.rename(fh.name, options.output)
            # put a dummy file on disk so NamedTemporaryFile does not
            # complain at exit. This is for python 2.4 (delete=True
            # came in python 2.6)
            open(fh.name, 'w')
            fh.close()
    end_write_t = time.time()

    # all done now save state.
    #
    # If using ipc, save dc as part of meta information to ipc_file.
    # Otherwise, save dc shelve object and save remaining meta to
    # cache_file.
    meta = {}
    if options.ipc_file is None:
        start_close_t = time.time()
        dc.close()
        end_close_t = time.time()

        header = {"version": VERSION,
                  "initial run": initial_run,
                  "timestamp": datetime.datetime.today(),
                  "args": args,
                  "options": options,
                  "ndir": len(hot),
                  "close dur": end_close_t - start_close_t,
                  "scan dur": end_scan_t - start_scan_t,
                  "write dur": end_write_t - start_write_t,
                  "pickle size": os.stat(cache_file)[stat.ST_SIZE],
                  "shelve size": sum(os.stat(f)[stat.ST_SIZE] 
                                     for f in glob.glob(shelve_file + '*'))
                  }
        meta_f = cache_file
        meta["header"] = header
        meta["hot"] = hot
    else:
        # save only new information as recorded in 'dc'
        _hot = {}
        meta_f = options.ipc_file
        meta["dc"] = dc
        meta["hot"] = dict(zip(dc.keys(), [hot[k] for k in dc]))

    # dump to file atomicly
    fh = tempfile.NamedTemporaryFile(dir=dirname(meta_f))
    pickle_dump(meta, fh, -1)
    fh.flush()
    os.chmod(fh.name, options.output_file_mode)
    os.rename(fh.name, meta_f)
    # put a dummy file on disk so NamedTemporaryFile does not complain
    # at exit. This is for python 2.4 (delete=True came in python 2.6)
    open(fh.name, 'w')
    fh.close()
